{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58cb2206",
   "metadata": {},
   "source": [
    "# Working with large virtual chemical libraries: Part 3 - Thompson Sampling for Classification\n",
    "\n",
    "---\n",
    "\n",
    "Only about 18 months after I wrote the first one, this is part 3 of a three post series on working with large chemical libraries. The notebook used to create this post and all the files can be found in this [github repo](https://github.com/jonswain/TS-for-classification).\n",
    "\n",
    "The first post in this series on active learning can be [found here](https://jonswain.github.io/active%20learning/ai/cheminformatics/data%20science/machine%20learning/ultra-large%20libraries/2024/05/18/ultra-large-libraries-part-1.html).\n",
    "\n",
    "The second post in this series on genetic algorithms can be [found here](https://jonswain.github.io/ai/cheminformatics/data%20science/genetic%20algorithms/ultra-large%20libraries/2025/01/02/ultra-large-libraries-part-2.html).\n",
    "\n",
    "---\n",
    "\n",
    "## Chemical space and combinatorial libraries\n",
    "\n",
    "In the first two parts of this series, I've discussed how chemical space is vast. The estimates of the size of theoretical chemical space, ranging from $10^{50}$ to $10^{80}$ possible compounds, vastly exceeds our practical screening capabilities, even when constrained to commercial combinatorial libraries like Enamine REAL, which contains tens of billions of compounds. As previously mentioned, even with a fast scoring function of one second per compound, exhaustively screening a billion-compound library would take over 31 years. This fundamental limitation necessitates the use of intelligent search strategies to efficiently navigate and identify active molecules within these immense virtual spaces.\n",
    "\n",
    "## Thompson Sampling\n",
    "\n",
    "This whole series was inspired by a paper on searching ultra-large chemical spaces using Thompson Sampling by Pat Walters et al., [\"Thompson Samplingâ”€An Efficient Method for Searching Ultralarge Synthesis on Demand Databases\"](https://pubs.acs.org/doi/10.1021/acs.jcim.3c01790).\n",
    "\n",
    "[Thompson Sampling](https://en.wikipedia.org/wiki/Thompson_sampling) is a powerful reinforcement learning technique used to address the [\"multi-armed bandit problem\"](https://en.wikipedia.org/wiki/Multi-armed_bandit). In the multi-armed bandit problem, a gambler must choose which of a row of slot machines (multi-armed bandits) to play, aiming to maximize total returns by finding the most rewarding machine. This requires balancing exploration (testing all machines to estimate their true return rate) and exploitation (playing the machine currently believed to be the best).\n",
    "\n",
    "Thompson Sampling aims to maximize returns by modeling a **posterior belief distribution** (the probability distribution of potential rewards) for each slot machine. This is usually initially generated by random sampling to build up a distribution. During each round of Thompson Sampling, for each machine, a value is randomly sampled from its posterior belief distribution, and the machine with the highest sampled value is selected for the next trial (the arm on the slot machine is pulled). The outcome (reward) from that trial is then used to update the machine's belief distribution, making it more accurate. The randomness associated from sampling the belief distribution for each machine allows exploration of the returns for each machine, but because the machines with genuinely higher returns will have distributions that generally return higher numbers when sampled, they are trialed more frequently, allowing for sufficient exploitation. \n",
    "\n",
    "### Thompson Sampling for virtual chemistry screening\n",
    "\n",
    "In the paper by Walters et al., the \"slot machines\" are the individual building blocks used in a virtual reaction. These blocks are combined to form a virtual molecule, which is then assessed using a scoring function (the \"reward\"). Each building block has a belief distribution for the scores associated with compounds containing it, which is updated after the compound is scored.\n",
    "\n",
    "The authors are seeking to maximize a continuous scoring function. They modeled the reward distribution using a Normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ with mean $\\mu$ and standard deviation $\\sigma$:\n",
    "$$X \\sim \\mathcal{N}(\\mu, \\sigma^2)$$\n",
    "\n",
    "After each trial, a Bayesian update was performed on the parameters ($\\mu$ and $\\sigma$) of the belief distribution for each building block found in the scored compound. Using this method, the authors showed that Thompson Sampling could identify more than half of the top 100 molecules from a docking-based virtual screen of 335 million molecules by evaluating just 1% of the data set.\n",
    "\n",
    "## Thompson Sampling for classification\n",
    "\n",
    "What if we've built a classification machine learning model to predict compound activity (Active=1, Inactive=0), but we're not able to exhaustatively screen the entire library, can we use Thompson Sampling to find active compounds from our library? We can no longer model our reward distributions using the Normal distribution, as our activity labels are just 0 or 1, and we can't have values greater than 1 or less than 0. Instead we need to use the [Beta Distribution](https://en.wikipedia.org/wiki/Beta_distribution).\n",
    "\n",
    "The underlying event when scoring each compound is a [Bernoulli trial](https://en.wikipedia.org/wiki/Bernoulli_trial), returning either a success (1) or a failure (0), so we can use the Bernoulli distribution to model the probability the compound containing the building block is active with probability $p$. For the Bayesian statistics required for Thompson Samping, we need to model the uncertainty about $p$. The Beta distribution is the **conjugate prior probability distribution** for the Bernoulli distribution, making it the perfect choice to model the uncertainty about $p$. The Beta distribution is a family of continuous probability distributions defined between 0 and 1, defined by two parameters: alpha ($\\alpha$) and beta ($\\beta$). Alpha is related to the number of successes (active compounds), and beta to the number of failures (inactive compounds) in the Bernoulli trials.\n",
    "\n",
    "Traditionally, $\\alpha$ is updated by adding 1 for a success, and $\\beta$ is updated by adding 1 for a failure. However, since a classification ML model provides a probability prediction ($P_{\\text{pred}}$), we can use a more informative soft-update approach: we sum $P_{\\text{pred}}$ for $\\alpha$ (representing partial success evidence), and sum $1 - P_{\\text{pred}}$ for $\\beta$ (representing partial failure evidence).\n",
    "\n",
    "## Data and machine learning classification model\n",
    "\n",
    "As with all the other posts in this series, the smi files used here were borrowed from Pat Walters repository on [Thompson sampling](https://github.com/PatWalters/TS). These files contain three different types of building blocks, with 100 examples in each. Reacting all combinations of these building blocks creates a virtual library of one million compounds. The other dataset is a collection of chemical compounds labelled with their hERG activity from [Cai et al](https://pubs.acs.org/doi/10.1021/acs.jcim.8b00769). This dataset is used to train a classification machine learning model to predict hERG activity. This model can then be used with Thompson Sampling to select the compounds from the virtual library with the highest predicted values for hERG activity. We generally want to minimise hERG activity, but this could be replaced with a QSAR classification model to predict on-target activity.\n",
    "\n",
    "---\n",
    "\n",
    "## Imports\n",
    "\n",
    "First, we need to import the libraries we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e609ee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from dataclasses import InitVar, dataclass, field\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem.rdchem import Mol\n",
    "from scipy.stats import beta\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"[%(asctime)s] %(levelname)s: %(message)s\",\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91b9d80",
   "metadata": {},
   "source": [
    "## Useful classes for storing data\n",
    "\n",
    "I've created a few Python classes to store data and make running the experiment easier.\n",
    "\n",
    "The first is a `Synthon` class, which stores the information for a building block. The alpha and beta attributes describe the belief distribution. These are set during the experiment initialization, are used to create the distribution during Thompson Sampling, and are updated with each new score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5353c803",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Synthon:\n",
    "    \"\"\"A building block for creating a compound.\n",
    "\n",
    "    Attributes:\n",
    "        idx (int): Index of the synthon.\n",
    "        smiles (str): SMILES representation of the synthon.\n",
    "        mol (Mol): RDKit molecule object, initialized from SMILES.\n",
    "        initial_scores (list[float]): Initial activity scores for the synthon.\n",
    "        alpha (float): Alpha parameter for distribution. Default is 1.0.\n",
    "        beta (float): Beta parameter for distribution. Default is 1.0.\n",
    "    \"\"\"\n",
    "\n",
    "    idx: int\n",
    "    smiles: str\n",
    "    mol: Mol = field(init=False)\n",
    "    initial_scores: list[float] = field(default_factory=list)\n",
    "    alpha: float = field(default=1.0)\n",
    "    beta: float = field(default=1.0)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.mol = Chem.MolFromSmiles(self.smiles)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Synthon(idx={self.idx}, smiles='{self.smiles}, \\\n",
    "        alpha={self.alpha}, beta={self.beta})\"\n",
    "\n",
    "    def initialize_distribution(self):\n",
    "        \"\"\"Initialize the distribution parameters (alpha and beta) for the synthon.\n",
    "\n",
    "        The activity score distributions are modelled using a Beta distribution, which\n",
    "        has two shape parameters: alpha and beta. If there are no initial scores,\n",
    "        default values are assigned. If there are initial scores, the parameters can be\n",
    "        estimated from the data.\n",
    "        \"\"\"\n",
    "        if not self.initial_scores:\n",
    "            log.warning(f\"Synthon {self.idx} initialized with Beta(1.0, 1.0) prior.\")\n",
    "        else:\n",
    "            self.alpha = 1.0 + sum(self.initial_scores)\n",
    "            self.beta = 1.0 + sum(1 - score for score in self.initial_scores)\n",
    "            self.alpha = max(1.0, self.alpha)\n",
    "            self.beta = max(1.0, self.beta)\n",
    "        self.initial_scores = []\n",
    "\n",
    "    def sample_logit(self) -> float:\n",
    "        \"\"\"Draws a sample from the current Beta distribution and returns its logit.\n",
    "\n",
    "        Logit is defined as log(p / (1 - p)), where p is the sampled probability. A\n",
    "        small epsilon is used to avoid log(0) issues.\n",
    "        \"\"\"\n",
    "        p_sample = beta.rvs(self.alpha, self.beta)\n",
    "        epsilon = 1e-9\n",
    "        p_sample = max(epsilon, min(1 - epsilon, p_sample))\n",
    "        return math.log(p_sample / (1 - p_sample))\n",
    "\n",
    "    def update_distribution(self, p_pred: float):\n",
    "        \"\"\"Update the distribution parameters with a new activity score.\n",
    "\n",
    "        This uses a softened update concept:\n",
    "\n",
    "            alpha_new = alpha_old + P_pred\n",
    "            beta_new = beta_old + (1 - P_pred)\n",
    "\n",
    "        Args:\n",
    "            p_pred (float): The new activity score to incorporate.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the distribution parameters are not initialized.\n",
    "        \"\"\"\n",
    "        if self.alpha is None or self.beta is None:\n",
    "            raise ValueError(\n",
    "                \"Distribution parameters must be initialized before updating.\"\n",
    "            )\n",
    "        self.alpha += p_pred\n",
    "        self.beta += 1 - p_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ac8ef3",
   "metadata": {},
   "source": [
    "The second is a `Compound` class, used to store information and data about enumerated virtual compounds, including the score from the classification machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Compound:\n",
    "    \"\"\"A virtual compound created by reacting three synthons.\n",
    "\n",
    "    Attributes:\n",
    "        axis_1_synthon (Synthon): Synthon from axis 1.\n",
    "        axis_2_synthon (Synthon): Synthon from axis 2.\n",
    "        axis_3_synthon (Synthon): Synthon from axis 3.\n",
    "        compound_id (str): Unique identifier for the compound.\n",
    "        rnx (InitVar[AllChem.ChemicalReaction]): RDKit reaction object used for synthesis.\n",
    "        mol (Mol | None): RDKit molecule object of the compound.\n",
    "        fp (list[int] | None): Molecular fingerprint of the compound.\n",
    "        score (float | None): Activity score of the compound.\n",
    "    \"\"\"\n",
    "\n",
    "    axis_1_synthon: Synthon\n",
    "    axis_2_synthon: Synthon\n",
    "    axis_3_synthon: Synthon\n",
    "    compound_id: str = field(init=False)\n",
    "    rnx: InitVar[AllChem.ChemicalReaction]\n",
    "    mol: Mol | None = field(init=False)\n",
    "    fp: list[int] | None = field(init=False, default=None)\n",
    "    score: float | None = None\n",
    "\n",
    "    def __post_init__(self, rnx):\n",
    "        self.compound_id = (\n",
    "            f\"{self.axis_1_synthon.idx}-\"\n",
    "            f\"{self.axis_2_synthon.idx}-\"\n",
    "            f\"{self.axis_3_synthon.idx}\"\n",
    "        )\n",
    "        products = rnx.RunReactants(\n",
    "            [self.axis_1_synthon.mol, self.axis_2_synthon.mol, self.axis_3_synthon.mol]\n",
    "        )\n",
    "        if products:\n",
    "            self.mol = products[0][0]\n",
    "            Chem.SanitizeMol(self.mol)\n",
    "            self.fp = list(AllChem.GetMorganFingerprintAsBitVect(self.mol, radius=2))\n",
    "        else:\n",
    "            log.warning(\"Virtual synthesis failed for compound %s\", self.compound_id)\n",
    "            self.mol = None\n",
    "            self.score = None\n",
    "            self.fp = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Compound(id='{self.compound_id}', score={self.score})\"\n",
    "\n",
    "    @property\n",
    "    def synthons(self):\n",
    "        \"\"\"Return the list of synthons that make up the compound.\"\"\"\n",
    "        return [self.axis_1_synthon, self.axis_2_synthon, self.axis_3_synthon]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedbcfce",
   "metadata": {},
   "source": [
    "The final is a `TSExperiment` class, used to manage the overall experiment. This takes in lists of SMILES strings to be used to construct the virtual compounds. The `initialize_distributions` method creates a initial sample of compounds, uses the machine learning model to score them, then uses these scores to initialize the belief distributions for each building block. The `run_ts_loop` method then uses Thompson Sampling to select compounds, scores them, and updates the belief distributions, aiming to select higher performing compounds each round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608e7900",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TSExperiment:\n",
    "    \"\"\"A Thompson Sampling experiment to maximise a classification prediction.\n",
    "\n",
    "    Attributes:\n",
    "        smiles_1 (InitVar[list[str]]): List of SMILES for axis 1 synthons.\n",
    "        smiles_2 (InitVar[list[str]]): List of SMILES for axis 2 synthons.\n",
    "        smiles_3 (InitVar[list[str]]): List of SMILES for axis 3 synthons.\n",
    "        reaction_smarts (str): SMARTS string defining the reaction.\n",
    "        ml_model (BaseEstimator): Pre-trained machine learning model for scoring.\n",
    "        axis_1 (list[Synthon]): List of Synthon objects for axis 1.\n",
    "        axis_2 (list[Synthon]): List of Synthon objects for axis 2.\n",
    "        axis_3 (list[Synthon]): List of Synthon objects for axis 3.\n",
    "        rnx (AllChem.ChemicalReaction): RDKit reaction object.\n",
    "        all_compounds (dict[str, Compound]): Dictionary of compounds that have been scored.\n",
    "        compounds_by_batch (dict[int, dict[str, Compound]]): Dictionary of all generated\n",
    "                                                             compounds each round.\n",
    "    \"\"\"\n",
    "\n",
    "    smiles_1: InitVar[list[str]]\n",
    "    smiles_2: InitVar[list[str]]\n",
    "    smiles_3: InitVar[list[str]]\n",
    "    reaction_smarts: str\n",
    "    ml_model: BaseEstimator\n",
    "    axis_1: list[Synthon] = field(init=False)\n",
    "    axis_2: list[Synthon] = field(init=False)\n",
    "    axis_3: list[Synthon] = field(init=False)\n",
    "    rnx: AllChem.ChemicalReaction = field(init=False)\n",
    "    all_compounds: dict[str, Compound] = field(default_factory=dict)\n",
    "    compounds_by_batch: dict[int, dict[str, Compound]] = field(default_factory=dict)\n",
    "\n",
    "    def __post_init__(self, smiles_1, smiles_2, smiles_3):\n",
    "        log.info(\"Initializing Thompson Sampling experiment\")\n",
    "        self.axis_1 = [\n",
    "            Synthon(idx=i, smiles=smiles) for i, smiles in enumerate(smiles_1)\n",
    "        ]\n",
    "        self.axis_2 = [\n",
    "            Synthon(idx=i, smiles=smiles) for i, smiles in enumerate(smiles_2)\n",
    "        ]\n",
    "        self.axis_3 = [\n",
    "            Synthon(idx=i, smiles=smiles) for i, smiles in enumerate(smiles_3)\n",
    "        ]\n",
    "        self.rnx = Chem.AllChem.ReactionFromSmarts(self.reaction_smarts)\n",
    "\n",
    "    def initialize_distributions(self, n_initial_scores: int):\n",
    "        \"\"\"Initialize the synthon distributions by scoring random compounds.\n",
    "\n",
    "        Args:\n",
    "            n_initial_scores (int): Number of random compounds to score for initialization.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the number of initial scores exceeds the total possible compounds.\n",
    "        \"\"\"\n",
    "        n_possible_compounds = math.prod(\n",
    "            [len(self.axis_1), len(self.axis_2), len(self.axis_3)]\n",
    "        )\n",
    "        percent_coverage = (n_initial_scores / n_possible_compounds) * 100\n",
    "        log.info(\n",
    "            \"Initializing synthon distributions with %s random compounds\",\n",
    "            \"{:,}\".format(n_initial_scores),\n",
    "        )\n",
    "        if percent_coverage > 100:\n",
    "            raise ValueError(\n",
    "                f\"Requested initial scores ({n_initial_scores}) exceed total possible compounds ({n_possible_compounds}).\"\n",
    "            )\n",
    "\n",
    "        log.info(\n",
    "            \"This represents %.1f%% of the total possible compound space (%s compounds).\",\n",
    "            percent_coverage,\n",
    "            \"{:,}\".format(n_possible_compounds),\n",
    "        )\n",
    "        initial_compound_ids = set(self.all_compounds.keys())\n",
    "        compounds_to_score: list[Compound] = []\n",
    "\n",
    "        pbar = tqdm(total=n_initial_scores, desc=\"Generating initial compounds\")\n",
    "        while len(compounds_to_score) < n_initial_scores:\n",
    "            c1, c2, c3 = (\n",
    "                random.choice(axis) for axis in [self.axis_1, self.axis_2, self.axis_3]\n",
    "            )\n",
    "            compound_id = f\"{c1.idx}-{c2.idx}-{c3.idx}\"\n",
    "\n",
    "            if compound_id not in initial_compound_ids:\n",
    "                new_compound = Compound(\n",
    "                    axis_1_synthon=c1,\n",
    "                    axis_2_synthon=c2,\n",
    "                    axis_3_synthon=c3,\n",
    "                    rnx=self.rnx,\n",
    "                )\n",
    "                self.all_compounds[compound_id] = new_compound\n",
    "                initial_compound_ids.add(compound_id)\n",
    "                compounds_to_score.append(new_compound)\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "\n",
    "        self.compounds_by_batch[0] = {c.compound_id: c for c in compounds_to_score}\n",
    "\n",
    "        log.info(\"Scoring initial compounds\")\n",
    "        self.score_compounds(compounds_to_score)\n",
    "        for compound in compounds_to_score:\n",
    "            for synthon in compound.synthons:\n",
    "                if compound.score is not None:\n",
    "                    synthon.initial_scores.append(compound.score)\n",
    "\n",
    "        for axis in [self.axis_1, self.axis_2, self.axis_3]:\n",
    "            for synthon in axis:\n",
    "                synthon.initialize_distribution()\n",
    "\n",
    "    def _randomly_sample_compound(self) -> Compound:\n",
    "        \"\"\"Return a randomly sampled compound from the synthon axes.\"\"\"\n",
    "        return Compound(\n",
    "            axis_1_synthon=random.choice(self.axis_1),\n",
    "            axis_2_synthon=random.choice(self.axis_2),\n",
    "            axis_3_synthon=random.choice(self.axis_3),\n",
    "            rnx=self.rnx,\n",
    "        )\n",
    "\n",
    "    def score_compounds(self, compounds: list[Compound]) -> None:\n",
    "        \"\"\"Score a compound using the ML model.\"\"\"\n",
    "        compounds_to_score = [\n",
    "            c for c in compounds if c.score is None and c.mol is not None\n",
    "        ]\n",
    "        if len(compounds_to_score) > 0:\n",
    "            X = pd.DataFrame(\n",
    "                [c.fp for c in compounds_to_score],\n",
    "                columns=[f\"fp_{x}\" for x in range(len(compounds_to_score[0].fp))],\n",
    "            )\n",
    "            predicted_probas = self.ml_model.predict_proba(X)[:, 1]\n",
    "            for compound, proba in zip(compounds_to_score, predicted_probas):\n",
    "                compound.score = proba\n",
    "\n",
    "    def _thompson_sample_compound(self) -> Compound:\n",
    "        sampled_synthons_1 = [\n",
    "            (synthon, synthon.sample_logit()) for synthon in self.axis_1\n",
    "        ]\n",
    "        sampled_synthons_2 = [\n",
    "            (synthon, synthon.sample_logit()) for synthon in self.axis_2\n",
    "        ]\n",
    "        sampled_synthons_3 = [\n",
    "            (synthon, synthon.sample_logit()) for synthon in self.axis_3\n",
    "        ]\n",
    "\n",
    "        best_1 = max(sampled_synthons_1, key=lambda x: x[1])\n",
    "        best_2 = max(sampled_synthons_2, key=lambda x: x[1])\n",
    "        best_3 = max(sampled_synthons_3, key=lambda x: x[1])\n",
    "        compound_id = f\"{best_1[0].idx}-{best_2[0].idx}-{best_3[0].idx}\"\n",
    "\n",
    "        if compound_id in self.all_compounds:\n",
    "            return self.all_compounds[compound_id]\n",
    "\n",
    "        new_compound = Compound(\n",
    "            axis_1_synthon=best_1[0],\n",
    "            axis_2_synthon=best_2[0],\n",
    "            axis_3_synthon=best_3[0],\n",
    "            rnx=self.rnx,\n",
    "        )\n",
    "        self.all_compounds[compound_id] = new_compound\n",
    "        return new_compound\n",
    "\n",
    "    def run_ts_loop(self, n_iterations: int, scoring_batch_size: int = 100) -> None:\n",
    "        \"\"\"Run the Thompson Sampling loop for a specified number of iterations.\"\"\"\n",
    "        log.info(\n",
    "            \"Starting Thompson Sampling for %s iterations\", \"{:,}\".format(n_iterations)\n",
    "        )\n",
    "        n_possible_compounds = math.prod(\n",
    "            [len(self.axis_1), len(self.axis_2), len(self.axis_3)]\n",
    "        )\n",
    "        percent_coverage = (n_iterations / n_possible_compounds) * 100\n",
    "        log.info(\n",
    "            \"This represents %.1f%% of the total possible compound space (%s compounds).\",\n",
    "            percent_coverage,\n",
    "            \"{:,}\".format(n_possible_compounds),\n",
    "        )\n",
    "        n_batches = n_iterations // scoring_batch_size\n",
    "        for i in tqdm(range(n_batches), desc=\"Thompson Sampling batches\"):\n",
    "            batch_compounds = [\n",
    "                self._thompson_sample_compound() for _ in range(scoring_batch_size)\n",
    "            ]\n",
    "            compounds_to_score = [\n",
    "                c for c in batch_compounds if c.score is None and c.mol is not None\n",
    "            ]\n",
    "            self.score_compounds(compounds_to_score)\n",
    "            for compound in batch_compounds:\n",
    "                if compound.score is not None:\n",
    "                    for synthon in compound.synthons:\n",
    "                        synthon.update_distribution(compound.score)\n",
    "            self.compounds_by_batch[i + 1] = {c.compound_id: c for c in batch_compounds}\n",
    "        log.info(\"Thompson Sampling loop finished.\")\n",
    "\n",
    "    def return_top_compounds(self, n_top: int) -> list[Compound]:\n",
    "        \"\"\"Return the top N scored compounds from the entire set.\"\"\"\n",
    "        scored = [c for c in self.all_compounds.values() if c.score is not None]\n",
    "        return sorted(\n",
    "            scored,\n",
    "            key=lambda c: c.score,\n",
    "            reverse=True,\n",
    "        )[:n_top]\n",
    "\n",
    "    def summarise_results(self) -> pd.DataFrame:\n",
    "        \"\"\"Summarise the results of the experiment into a DataFrame.\"\"\"\n",
    "        records = []\n",
    "        for batch_idx, compounds in self.compounds_by_batch.items():\n",
    "            for compound in compounds.values():\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"batch\": batch_idx,\n",
    "                        \"compound_id\": compound.compound_id,\n",
    "                        \"score\": compound.score,\n",
    "                    }\n",
    "                )\n",
    "        return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65f1e6",
   "metadata": {},
   "source": [
    "## Some other useful functions\n",
    "\n",
    "A couple quick functions to load the SMILES data and train the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db27f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification_model(\n",
    "    data_path: Path, smiles_col: str, activity_col: str\n",
    ") -> BaseEstimator:\n",
    "    \"\"\"Train the classification model to predict activity.\"\"\"\n",
    "    # TODO: Calibrated model\n",
    "    # from sklearn.calibration import CalibratedClassifierCV\n",
    "    # base_model = RandomForestClassifier(random_state=42)\n",
    "    # model = CalibratedClassifierCV(base_model, method='isotonic', cv=5)\n",
    "    # model.fit(X_train, y_train)\n",
    "    # return model\n",
    "    log.info(\"Training classification model using data from %s\", data_path)\n",
    "    training_data = pd.read_csv(data_path)\n",
    "    mols = [Chem.MolFromSmiles(smi) for smi in training_data[smiles_col]]\n",
    "    fps = [list(AllChem.GetMorganFingerprintAsBitVect(mol, radius=2)) for mol in mols]\n",
    "    X_train = pd.DataFrame(fps, columns=[f\"fp_{x}\" for x in range(len(fps[0]))])\n",
    "    y_train = training_data[activity_col]\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "\n",
    "def read_smi_file(file_path: Path) -> list[str]:\n",
    "    \"\"\"Read SMILES strings from a .smi file.\"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return [line.split()[0] for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733573fc",
   "metadata": {},
   "source": [
    "## Running the Thompson Sampling experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0eb8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# herg_dataset_cai.csv contains columns \"smiles\" and \"activity\"\n",
    "ml_model_training_data = Path(\"data/herg_dataset_cai.csv\")\n",
    "ml_model = train_classification_model(\n",
    "    data_path=ml_model_training_data,\n",
    "    smiles_col=\"smiles\",\n",
    "    activity_col=\"activity\",\n",
    ")\n",
    "\n",
    "# Create and run experiment\n",
    "ts_start = time.time()\n",
    "aminobenzoic_acids = read_smi_file(Path(\"data/aminobenzoic_100.smi\"))\n",
    "primary_amines = read_smi_file(Path(\"data/primary_amines_100.smi\"))\n",
    "carboxylic_acids = read_smi_file(Path(\"data/carboxylic_acids_100.smi\"))\n",
    "experiment = TSExperiment(\n",
    "    smiles_1=aminobenzoic_acids,\n",
    "    smiles_2=primary_amines,\n",
    "    smiles_3=carboxylic_acids,\n",
    "    reaction_smarts=\"N[c:4][c:3]C(O)=O.[#6:1][NH2].[#6:2]C(=O)[OH]>>[C:2]c1n[c:4][c:3]c(=O)n1[C:1]\",\n",
    "    ml_model=ml_model,\n",
    ")\n",
    "experiment.initialize_distributions(n_initial_scores=10000)\n",
    "experiment.run_ts_loop(n_iterations=50000, scoring_batch_size=1000)\n",
    "ts_end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458f0428",
   "metadata": {},
   "source": [
    "## Comparing to full library enumeration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2153037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enumerature_entire_library() -> pd.DataFrame:\n",
    "    reaction_smarts = (\n",
    "        \"N[c:4][c:3]C(O)=O.[#6:1][NH2].[#6:2]C(=O)[OH]>>[C:2]c1n[c:4][c:3]c(=O)n1[C:1]\"\n",
    "    )\n",
    "    rxn = AllChem.ReactionFromSmarts(reaction_smarts)\n",
    "\n",
    "    bb_types = (\"aminobenzoic\", \"primary_amines\", \"carboxylic_acids\")\n",
    "    building_blocks = {}\n",
    "    for bb in bb_types:\n",
    "        mols = []\n",
    "        with open(Path(f\"data/{bb}_100.smi\"), \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                smiles, _ = line.split()\n",
    "                mol = Chem.MolFromSmiles(smiles)\n",
    "                if mol is not None:\n",
    "                    mols.append(mol)\n",
    "        building_blocks[bb] = mols\n",
    "\n",
    "    product_list = []\n",
    "    for i, j, k in tqdm(\n",
    "        product(\n",
    "            range(len(building_blocks[\"aminobenzoic\"])),\n",
    "            range(len(building_blocks[\"primary_amines\"])),\n",
    "            range(len(building_blocks[\"carboxylic_acids\"])),\n",
    "        ),\n",
    "        total=len(building_blocks[\"aminobenzoic\"])\n",
    "        * len(building_blocks[\"primary_amines\"])\n",
    "        * len(building_blocks[\"carboxylic_acids\"]),\n",
    "    ):\n",
    "\n",
    "        reagent_mol_tuple = (\n",
    "            building_blocks[\"aminobenzoic\"][i],\n",
    "            building_blocks[\"primary_amines\"][j],\n",
    "            building_blocks[\"carboxylic_acids\"][k],\n",
    "        )\n",
    "        products = rxn.RunReactants(reagent_mol_tuple)\n",
    "\n",
    "        reagent_mol_tuple = (\n",
    "            building_blocks[\"aminobenzoic\"][i],\n",
    "            building_blocks[\"primary_amines\"][j],\n",
    "            building_blocks[\"carboxylic_acids\"][k],\n",
    "        )\n",
    "        products = rxn.RunReactants(reagent_mol_tuple)\n",
    "        if products:\n",
    "            Chem.SanitizeMol(products[0][0])\n",
    "            product_list.append(\n",
    "                {\n",
    "                    \"idx\": f\"{i}-{j}-{k}\",\n",
    "                    \"mol\": products[0][0],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    library = pd.DataFrame(\n",
    "        product_list,\n",
    "        index=[Chem.MolToSmiles(m[\"mol\"]) for m in product_list],\n",
    "    )\n",
    "    library.index.name = \"smiles\"\n",
    "    return library\n",
    "\n",
    "\n",
    "full_library_start = time.time()\n",
    "library = enumerature_entire_library()\n",
    "library[\"herg_predictions\"] = np.nan\n",
    "batch_size = 50000\n",
    "for start_idx in tqdm(range(0, len(library), batch_size)):\n",
    "    end_idx = min(start_idx + batch_size, len(library))\n",
    "    library.iloc[start_idx:end_idx, library.columns.get_loc(\"herg_predictions\")] = (\n",
    "        ml_model.predict_proba(\n",
    "            create_morgan_fingerprints(list(library[\"mol\"].iloc[start_idx:end_idx]))\n",
    "        )[:, 1]\n",
    "    )\n",
    "library[\"herg_rank\"] = library[\"herg_predictions\"].rank(ascending=False)\n",
    "full_library_end = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49571eca",
   "metadata": {},
   "source": [
    "## Evaluation of performance\n",
    "\n",
    "1. Cumulative Regret\n",
    "\n",
    "The standard metric in Reinforcement Learning, measuring the performance lost by not choosing the truly optimal action at every step. Regret is the the reward (score) of the single best compound in the entire library minus the score of the compound selected by TS at iteration $t$. In this situation averaged across the batch. Hopefully converges to zero. Compare to random sampling.\n",
    "\n",
    "2. Time-to-Target (Efficacy)\n",
    "\n",
    "How many compounds need to be scored before TS discovers K unique compounds whose score is greater than or equal to a target threshold (e.g. a clinically relevant cutoff). Compared to random sampling.\n",
    "\n",
    "3. Area Under the Active Enrichment Curve (AUAEC)\n",
    "\n",
    "This is a powerful metric often used in virtual screening. It assesses the overall quality of the compounds prioritized by TS.\n",
    "\n",
    "Process:\n",
    " - At the end of the experiment, rank the compounds based on the order they were selected by TS (or just use the cumulative set of selected compounds).\n",
    " - For a given percentage of the screened library, calculate the percentage of true active compounds (compounds above a threshold) that were recovered.\n",
    " \n",
    " Metric: \n",
    " - The Area Under the Curve (AUC) of the plot where the x-axis is % Library Screened and the y-axis is % Actives Recovered.\n",
    " \n",
    " Interpretation: \n",
    " - An AUAEC close to 1.0 means TS prioritized active compounds perfectly. Comparing this AUC to the AUC achieved by a random selection strategy (which is typically 0.5 for binary classification) or an ML-model-only ranking shows the true value of the sampling strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46167800",
   "metadata": {},
   "outputs": [],
   "source": [
    "log.info(\"Thompson Sampling took %.1f minutes\", (ts_end - ts_start) / 60)\n",
    "log.info(\n",
    "    \"Full library enumeration and scoring took %.1f minutes\",\n",
    "    (full_library_end - full_library_start) / 60,\n",
    ")\n",
    "sns.scatterplot(experiment.summarise_results(), x=\"batch\", y=\"score\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TS-for-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
